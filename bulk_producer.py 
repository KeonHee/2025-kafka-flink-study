from confluent_kafka import Producer
import json
import time

conf = {
    'bootstrap.servers': 'localhost:9092',
    'acks': '1',
    'linger.ms': 10,
    'batch.num.messages': 1000,
    'compression.type': 'snappy'
}

producer = Producer(conf)

def delivery_report(err, msg):
    if err is not None:
        print(f"âŒ Delivery failed: {err}")
    else:
        print(f"âœ… Delivered to {msg.topic()} [{msg.partition()}]")

def generate_data(index):
    return {
        "id": index,
        "timestamp": int(time.time() * 1000),
        "value": f"message-{index}"
    }

def produce_bulk(topic: str, count: int):
    for i in range(count):
        data = generate_data(i)
        producer.produce(topic, json.dumps(data), callback=delivery_report)
        if i % 1000 == 0:
            print(f"ğŸ“¦ Sent {i} messages")
        producer.poll(0)
    producer.flush()
    print(f"âœ… All {count} messages sent to topic '{topic}'")

if __name__ == "__main__":
    topic_name = "bulk-topic"
    message_count = 100_000  # ë©”ì‹œì§€ ìˆ˜ ì¡°ì • ê°€ëŠ¥
    produce_bulk(topic_name, message_count)
